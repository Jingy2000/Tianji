{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# scape articles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "开始编号: 32\n",
      "已保存: page_032.txt\n",
      "已保存: page_033.txt\n",
      "未找到内容: http://www.ziyexing.com/luxun/luxun_zw_fen_03.htm\n",
      "已保存: page_034.txt\n",
      "已保存: page_035.txt\n",
      "已保存: page_036.txt\n",
      "已保存: page_037.txt\n",
      "已保存: page_038.txt\n",
      "已保存: page_039.txt\n",
      "已保存: page_040.txt\n",
      "已保存: page_041.txt\n",
      "已保存: page_042.txt\n",
      "已保存: page_043.txt\n",
      "已保存: page_044.txt\n",
      "已保存: page_045.txt\n",
      "已保存: page_046.txt\n",
      "已保存: page_047.txt\n",
      "已保存: page_048.txt\n",
      "已保存: page_049.txt\n",
      "已保存: page_050.txt\n",
      "已保存: page_051.txt\n",
      "已保存: page_052.txt\n",
      "已保存: page_053.txt\n",
      "已保存: page_054.txt\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import os\n",
    "import re\n",
    "\n",
    "# 创建保存文件夹\n",
    "output_dir = \"./corpus/raw_articles\"\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "# 获取当前文件夹下最大的现有文件编号\n",
    "existing_files = [f for f in os.listdir(output_dir) if f.startswith(\"page_\") and f.endswith(\".txt\")]\n",
    "if existing_files:\n",
    "    # 提取现有文件的编号，转换为整数后找到最大值\n",
    "    max_existing_num = max(int(re.search(r\"\\d+\", f).group()) for f in existing_files)\n",
    "else:\n",
    "    max_existing_num = 0  # 如果没有文件，编号从 0 开始\n",
    "\n",
    "start_index = max_existing_num + 1\n",
    "print(f\"开始编号: {start_index}\")\n",
    "\n",
    "# 设置起始和结束页码\n",
    "start_page = 1\n",
    "\n",
    "# 而已集\n",
    "# end_page = 31  \n",
    "# url_prefix = \"http://www.ziyexing.com/luxun/luxun_zw_eyj_\"\n",
    "\n",
    "# 坟\n",
    "end_page = 24  # 设置要爬取的最大页码，根据需要调整\n",
    "url_prefix = \"http://www.ziyexing.com/luxun/luxun_zw_fen_\"\n",
    "\n",
    "# 循环爬取各个页面\n",
    "for page_num in range(start_page, end_page + 1):\n",
    "    # URL格式化\n",
    "    url = f\"{url_prefix}{page_num:02}.htm\"\n",
    "    response = requests.get(url)\n",
    "    \n",
    "    if response.status_code == 200:\n",
    "        # 解析页面内容\n",
    "        soup = BeautifulSoup(response.content, 'html.parser')\n",
    "        # 提取标题\n",
    "        title = soup.title.string\n",
    "\n",
    "        # 提取正文内容\n",
    "        content_table = soup.find(\"p\", style=\"line-height: 150%\")\n",
    "        if content_table:\n",
    "            content_text = title + \"\\n\"\n",
    "            \n",
    "            for element in content_table.stripped_strings:\n",
    "                content_text += element + \"\\n\"\n",
    "            \n",
    "            # 保存到本地文件，文件名用页面编号\n",
    "            file_path = os.path.join(output_dir, f\"page_{start_index:03}.txt\")\n",
    "            with open(file_path, 'w', encoding='utf-8') as file:\n",
    "                file.write(content_text.strip())  # 去除多余空行\n",
    "            \n",
    "            print(f\"已保存: page_{start_index:03}.txt\")\n",
    "            start_index += 1\n",
    "\n",
    "        else:\n",
    "            print(f\"未找到内容: {url}\")\n",
    "    else:\n",
    "        print(f\"无法访问页面: {url}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# clean text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "已清洗并保存: page_053.txt\n",
      "已清洗并保存: page_047.txt\n",
      "已清洗并保存: page_046.txt\n",
      "已清洗并保存: page_052.txt\n",
      "已清洗并保存: page_044.txt\n",
      "已清洗并保存: page_050.txt\n",
      "已清洗并保存: page_051.txt\n",
      "已清洗并保存: page_045.txt\n",
      "已清洗并保存: page_041.txt\n",
      "已清洗并保存: page_054.txt\n",
      "已清洗并保存: page_040.txt\n",
      "已清洗并保存: page_042.txt\n",
      "已清洗并保存: page_043.txt\n",
      "已清洗并保存: page_030.txt\n",
      "已清洗并保存: page_024.txt\n",
      "已清洗并保存: page_018.txt\n",
      "已清洗并保存: page_019.txt\n",
      "已清洗并保存: page_025.txt\n",
      "已清洗并保存: page_031.txt\n",
      "已清洗并保存: page_027.txt\n",
      "已清洗并保存: page_033.txt\n",
      "已清洗并保存: page_032.txt\n",
      "已清洗并保存: page_026.txt\n",
      "已清洗并保存: page_022.txt\n",
      "已清洗并保存: page_036.txt\n",
      "已清洗并保存: page_037.txt\n",
      "已清洗并保存: page_023.txt\n",
      "已清洗并保存: page_009.txt\n",
      "已清洗并保存: page_035.txt\n",
      "已清洗并保存: page_021.txt\n",
      "已清洗并保存: page_020.txt\n",
      "已清洗并保存: page_034.txt\n",
      "已清洗并保存: page_008.txt\n",
      "已清洗并保存: page_011.txt\n",
      "已清洗并保存: page_005.txt\n",
      "已清洗并保存: page_039.txt\n",
      "已清洗并保存: page_038.txt\n",
      "已清洗并保存: page_004.txt\n",
      "已清洗并保存: page_010.txt\n",
      "已清洗并保存: page_006.txt\n",
      "已清洗并保存: page_012.txt\n",
      "已清洗并保存: page_013.txt\n",
      "已清洗并保存: page_007.txt\n",
      "已清洗并保存: page_003.txt\n",
      "已清洗并保存: page_017.txt\n",
      "已清洗并保存: page_016.txt\n",
      "已清洗并保存: page_002.txt\n",
      "已清洗并保存: page_028.txt\n",
      "已清洗并保存: page_014.txt\n",
      "已清洗并保存: page_001.txt\n",
      "已清洗并保存: page_015.txt\n",
      "已清洗并保存: page_029.txt\n",
      "已清洗并保存: page_048.txt\n",
      "已清洗并保存: page_049.txt\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import re\n",
    "\n",
    "# 读取爬取的数据文件夹\n",
    "input_dir = \"./corpus/raw_articles\"\n",
    "output_dir = \"./corpus/cleaned_articles\"\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "# 正则模式：匹配注释、特殊字符、数字标记\n",
    "pattern = re.compile(r\"[①②③④⑤⑥⑦⑧⑨⑩⒀⒁⒂⒃⒄⒅⒆⒇]|<\\d{2,}>|〔\\d{2,}〕｜（\\d{2,}）|【\\d{2,}】|［\\d{2,}］|〈\\d{2,}〉\") \n",
    "\n",
    "# clean and format the text\n",
    "for filename in os.listdir(input_dir):\n",
    "    if filename.endswith(\".txt\"):\n",
    "        input_path = os.path.join(input_dir, filename)\n",
    "        output_path = os.path.join(output_dir, filename)\n",
    "        \n",
    "        with open(input_path, 'r', encoding='utf-8') as file:\n",
    "            content = file.read()\n",
    "        \n",
    "        # 去除注释内容\n",
    "        if \"【注释】\" in content or \"【注】\" in content:\n",
    "            content = re.split(\"【注释】|【注】\", content)[0]\n",
    "        \n",
    "        # 去除特殊字符\n",
    "        content = pattern.sub(\"\", content)\n",
    "        \n",
    "        # 删除段落中的多余换行和缩进\n",
    "        content = re.sub(r'\\s{2,}', ' ', content)  # 删除多余空白\n",
    "        \n",
    "        # 格式化段落：确保段落分隔清晰，每段独立成行\n",
    "        formatted_content = \"\"\n",
    "        for line in content.splitlines():\n",
    "            if line and line.strip()[-1] not in [\",\", \":\", \";\", \"：\", \"；\", \"，\"]:\n",
    "                formatted_content += line.strip() + \"\\n\"\n",
    "            else:\n",
    "                formatted_content += line.strip()\n",
    "        \n",
    "        # 保存清洗后的内容\n",
    "        with open(output_path, 'w', encoding='utf-8') as file:\n",
    "            file.write(formatted_content.strip())  # 去除多余空行\n",
    "        \n",
    "        print(f\"已清洗并保存: {filename}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
